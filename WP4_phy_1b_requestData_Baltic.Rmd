---
title: "WP4_phy_1b_requestData_Baltic"
author: "Markus Lindh"
date: '2020-09-28'
output: html_document
---

# Global options

```{r global_options}
knitr::opts_chunk$set(echo = TRUE)
require(sf)
require(tidyverse)
require(httr)
downloadDir <- "data/raw_data"
dataDir <- "data/derived_data"
```

Need to  create folders "byTrait" and "byDataset"!

# Read geographic layers

```{r readlayers}
layerurl <- "http://geo.vliz.be/geoserver/MarineRegions/ows?service=WFS&version=1.0.0&request=GetFeature&typeName=MarineRegions:eez_iho&outputFormat=application/json"
regions <- sf::st_read(layerurl)

# read selected geographic layers for downloading
baltic <- read_delim("data/derived_data/baltic_regions.csv", delim = ";")
```

# Download data by geographic location and trait

```{r downloaddata}
beginDate<- "1995-01-01"
endDate <- "2020-05-31"

attributeID1 <- "phytoplankton"
attributeID2 <- "Phytoplankton"
attributeID3 <- NULL

# Full occurrence (selected columns)
for(ii in 1:length(baltic$mrgid)){
  mrgid <- baltic$mrgid[ii]
  print(paste("downloadingdata for", baltic$marregion[ii]))
  downloadURL <- paste0("http://geo.vliz.be/geoserver/wfs/ows?service=WFS&version=1.1.0&request=GetFeature&typeName=Dataportal%3Aeurobis-obisenv_full&resultType=results&viewParams=where%3A%28%28up.geoobjectsids+%26%26+ARRAY%5B", mrgid, "%5D%29%29+AND+%28%28observationdate+BETWEEN+%27", beginDate, "%27+AND+%27", endDate, "%27+%29%29+AND+aphiaid+IN+%28+SELECT+aphiaid+FROM+eurobis.taxa_attributes+WHERE+selectid+IN+%28%27", attributeID1, "%27%5C%2C%27", attributeID2, "%27%29%29%3Bcontext%3A0100&propertyName=datasetid%2Cdatecollected%2Cdecimallatitude%2Cdecimallongitude%2Ccoordinateuncertaintyinmeters%2Cscientificname%2Caphiaid%2Cscientificnameaccepted%2Cinstitutioncode%2Ccollectioncode%2Coccurrenceid%2Cscientificnameauthorship%2Cscientificnameid%2Ckingdom%2Cphylum%2Cclass%2Corder%2Cfamily%2Cgenus%2Csubgenus%2Caphiaidaccepted%2Cbasisofrecord%2Ceventid&outputFormat=csv")
  # downloadURL <- paste0("http://geo.vliz.be/geoserver/wfs/ows?service=WFS&version=1.1.0&request=GetFeature&typeName=Dataportal%3Aeurobis-obisenv_basic&resultType=results&viewParams=where%3A%28%28up.geoobjectsids+%26%26+ARRAY%5B", mrgid, "%5D%29%29+AND+%28%28observationdate+BETWEEN+%27", beginDate, "%27+AND+%27", endDate, "%27+%29%29+AND+aphiaid+IN+%28+SELECT+aphiaid+FROM+eurobis.taxa_attributes+WHERE+selectid+IN+%28%27", attributeID1, "%27%5C%2C%27", attributeID2, "%27%5C%2C%27", attributeID3, "%27%29%29%3Bcontext%3A0100&propertyName=datasetid%2Cdatecollected%2Cdecimallatitude%2Cdecimallongitude%2Ccoordinateuncertaintyinmeters%2Cscientificname%2Caphiaid%2Cscientificnameaccepted&outputFormat=csv")
  filename = paste0("region", baltic$mrgid[ii], ".csv")
  data <- read_csv(downloadURL)
  write_delim(data, file.path(downloadDir, "byTrait", filename), delim = ";")
}
```

# Create file list and save output

```{r filelistsave}
filelist <- list.files("data/raw_data/byTrait")
allDataExtra <- lapply(filelist, function(x) 
  read_delim(file.path("data", "raw_data/byTrait", x), 
             delim = ";", 
             col_types = "ccccccTnnlccccccccccccccc")) %>%
  set_names(sub(".csv", "", filelist)) %>%
  bind_rows(.id = "mrgid") %>%
  mutate(mrgid = sub("region", "", mrgid))

write_delim(allDataExtra, file.path(dataDir, "allDataExtra.csv"), delim = ";")
```

#Filter from downloaded data

```{r filterdownloaddata}
allDataExtra <- read_delim(file.path(dataDir, "allDataExtra.csv"), delim = ";")

datasetidsoi <- allDataExtra %>% distinct(datasetid) %>% 
  mutate(datasetid = sub('http://www.emodnet-biology.eu/data-catalog?module=dataset&dasid=', "", datasetid, fixed = T))

allDataExtra %>% distinct(scientificnameaccepted) %>% dim() # 523 species
allDataExtra %>% distinct(decimallatitude, decimallongitude) %>% dim() # 4786 localities
```

# Retrieve data by dataset

```{r retrievedata}
# Get dataset names from IMIS web page via web scraping
# function that gets the second node "b" from the website (reverse engineering..)
getDatasetName <- function(datasetid){
require(textreadr)
require(rvest)
  url <- paste0("https://www.vliz.be/en/imis?module=dataset&dasid=", datasetid)
  site = read_html(url)
  fnames <- rvest::html_nodes(site, "b")
  html_text(fnames)[2]
}
# get all names and urls from id's
datasetidsoi$name <- sapply(datasetidsoi$datasetid, getDatasetName, simplify = T)
datasetidsoi$url <- paste0("https://www.vliz.be/en/imis?module=dataset&dasid=", datasetidsoi$datasetid)

# save datasets ids to file
write_delim(datasetidsoi, file.path(dataDir, "allDatasets.csv"), delim = ";")
```

# Manual inspection of dataset names

```{r manualinspection}
paste(datasetidsoi$datasetid, datasetidsoi$name)
# OK [1] "785 Continuous Plankton Recorder (Phytoplankton)"
# OK [2] "2453 SHARK - Marine phytoplankton monitoring in Sweden since 1983"
# NOT OK [3] "787 Continuous Plankton Recorder (Zooplankton)"
# OK [4] "2722 PANGAEA - Data from various sources"
# OK [5] "4424 ICES Phytoplankton Community dataset"
# OK [6] "5664 Phytoplankton data for Danish marine monitoring (ODAM) from 1988 - 2016"
# OK [7] "2463 Polish Monitoring Programme - Monitoring of the Baltic Sea: phytoplankton"
# OK [8] "5724 Finnish Baltic Sea phytoplankton monitoring, KPLANK database"
# NOT OK [9] "5727 SHARK - Regional monitoring and monitoring projects of Epibenthos in Sweden since 1994"
# NOT OK [10] "2455 SHARK - National Epibenthos monitoring in Sweden since 1992"
# OK [11] "1985 NODC World Ocean Database 2001: Plankton Data"
# OK [12] "5840 IFCB110-SMHI: Imaging flow cytometry from SMHI in Tangesund 2016"
# NOT OK [13] "2501 Baltic Sea benthic meiofauna and macrofauna mid 1990s"
# OK [14] "6021 Phytoplankton composition, biomass and abundance in Estonian territorial waters 1993-2016"
# NOT OK [15] "2498 National ferrybox project - discrete sampling by IMWM"

# These we are certain of not containing phytoplankton
notOKdatasets <- c(787, 4412, 5759, 2756, 4687)
getDatasets <- datasetidsoi %>%
  filter(!datasetid %in% notOKdatasets)

beginDate<- "1995-01-01"
endDate <- "2020-05-31"

# added Baltic as roi (region of interest?), if that was intended?
roi <- read_delim(file.path(dataDir, "baltic_regions.csv"), delim = ";", )

for(ii in 1:length(roi$mrgid)){
  for(jj in 1:length(getDatasets$datasetid)){
    datasetid <- getDatasets$datasetid[jj]
    mrgid <- roi$mrgid[ii]
    print(paste("downloadingdata for", roi$marregion[ii], "and", getDatasets$datasetid[jj]))
    downloadURL <- paste0("https://geo.vliz.be/geoserver/wfs/ows?service=WFS&version=1.1.0&request=GetFeature&typeName=Dataportal%3Aeurobis-obisenv_full&resultType=results&viewParams=where%3A%28%28up.geoobjectsids+%26%26+ARRAY%5B", mrgid, "%5D%29%29+AND+datasetid+IN+(", datasetid, ");context%3A0100&propertyName=datasetid%2Cdatecollected%2Cdecimallatitude%2Cdecimallongitude%2Ccoordinateuncertaintyinmeters%2Cscientificname%2Caphiaid%2Cscientificnameaccepted%2Cinstitutioncode%2Ccollectioncode%2Coccurrenceid%2Cscientificnameauthorship%2Cscientificnameid%2Ckingdom%2Cphylum%2Cclass%2Corder%2Cfamily%2Cgenus%2Csubgenus%2Caphiaidaccepted%2Cbasisofrecord%2Ceventid&outputFormat=csv")
    data <- read_csv(downloadURL, guess_max = 100000)
    filename = paste0("region", roi$mrgid[ii], "_datasetid", datasetid,  ".csv")
    if(nrow(data) != 0){
      write_delim(data, file.path(downloadDir, "byDataset", filename), delim = ";")
    }
  }
}
```

# Edit, filter and save datasets to file

```{r modifydata}
filelist <- list.files("data/raw_data/byDataset")
all2Data <- lapply(filelist, function(x) 
  read_delim(file.path("data", "raw_data/byDataset", x), 
             delim = ";", 
             col_types = "ccccccTnnnccccccccccccccc"
  )
) %>%
  set_names(filelist) %>%
  bind_rows(.id = "fileID") %>%
  separate(fileID, c("mrgid", "datasetID"), "_") %>%
  mutate(mrgid = sub("[[:alpha:]]+", "", mrgid)) %>%
  mutate(datasetID = sub("[[:alpha:]]+", "", datasetID))
  # mutate(mrgid = sub("region", "", mrgid))

write_delim(all2Data, file.path(dataDir, "all2Data.csv"), delim = ";")

datasetidsoi <- all2Data %>% distinct(datasetid) %>% 
  mutate(datasetid = sub('http://www.emodnet-biology.eu/data-catalog?module=dataset&dasid=', "", datasetid, fixed = T))

all2Data %>% distinct(scientificnameaccepted) %>% dim() #  
all2Data %>% distinct(decimallatitude, decimallongitude) %>% dim() # 

# Output looks like 2021 unique species and 16148 unique locations. This was exactly the same numbers as before in the previous scripts we ran
```

## Reproducibility

```{r reproducibility}
# Date time
Sys.time()

# Here we store the session info for this script
sessioninfo::session_info()
```
